{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "midterm_project_2016320145.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdw9Mbe07jHN"
      },
      "source": [
        "Name: 이운규\n",
        "\n",
        "ID: 2016320145\n",
        "\n",
        "Link to your midterm project: https://colab.research.google.com/drive/1_ShNVJovC-oJ4nq52ObupMA0eo8geetV?usp=sharing\n",
        "\n",
        "Link to your evaluation script: https://colab.research.google.com/drive/1LlKl_8uCnjwjc5LMd0zm2O9oSr-Ub4lV?usp=sharing\n",
        "\n",
        "These links should be shared to the web without any permission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSP4WF3yr1-b",
        "outputId": "5025445c-d0b0-4c4e-c885-1bfc6e34fe7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sl\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "import torch\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "filename = '/content/drive/My Drive/뇌및머신러닝/train_data.csv'\n",
        "data = pd.read_csv(filename)\n",
        "\n",
        "# def split_train(data, train_ratio)\n",
        "#     indices = np.permutation(len(data))\n",
        "#     train_set_sizes = int(len(data)*train_ratio)\n",
        "#     train_indices1 = indices[:train_set_sizes]\n",
        "#     return data.iloc[train_indices]\n",
        "# mean_group = round(data[data.DX_bl ==0].ST102CV.mean(),2)\n",
        "# print(mean_group)\n",
        "\n",
        "\n",
        "#data['ST64CV'].unique()\n",
        "# np.where(np.isnan(X))\n",
        "\n",
        "# iris = load_iris()\n",
        "# logreg = LogisticRegression()\n",
        "\n",
        "# scores = cross_val_score(logreg, iris.data, iris.target, cv=10) # model, train, target, cross validation\n",
        "\n",
        "# print('cross-val-score \\n{}'.format(scores))\n",
        "\n",
        "# print('cross-val-score.mean \\n{:.3f}'.format(scores.mean()))\n",
        "\n",
        "# data['ST64CV'].unique()\n",
        "# data['ST64CV'].isnull().sum()\n",
        "# data['ST102CV'].unique()\n",
        "# print(data.groupby('DX_bl').size())\n",
        "\n",
        "# X = data.iloc[:,4:144]  # all rows, all the features and no labels\n",
        "# y = data.iloc[:,0 ]  # all rows, label only\n",
        "\n",
        "X=data.drop(columns=['DX_bl','ADAS11','ADAS13','MMSE'],axis=1) #Predictors\n",
        "X = X.interpolate(method='linear')\n",
        "X = X[['ST116CV', 'ST23CV', 'ST24CV', 'ST44CV', 'ST83CV', 'ST91CV', 'ST24TA',\n",
        "       'ST31TA', 'ST32TA', 'ST35TA', 'ST40TA', 'ST64TA', 'ST83TA', 'ST91TA',\n",
        "       'ST99TA']]\n",
        "#feature selection\n",
        "y=data['DX_bl'] #Response\n",
        "X.head()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "Encoder_X = LabelEncoder() \n",
        "for col in X.columns:\n",
        "    X[col] = Encoder_X.fit_transform(X[col])\n",
        "Encoder_y=LabelEncoder()\n",
        "y = Encoder_y.fit_transform(y)\n",
        "X.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#############################\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# X=scaler.fit_transform(X)\n",
        "# X\n",
        "# from sklearn.decomposition import PCA\n",
        "# pca = PCA()\n",
        "# pca.fit_transform(X)\n",
        "\n",
        "# covariance=pca.get_covariance() #covariance\n",
        "# explained_variance=pca.explained_variance_\n",
        "# explained_variance\n",
        "\n",
        "# with plt.style.context('dark_background'):\n",
        "#     plt.figure(figsize=(10, 10))\n",
        "    \n",
        "#     plt.bar(range(140), explained_variance, alpha=0.5, align='center', label='individual explained variance')\n",
        "#     plt.ylabel('Explained variance ratio')\n",
        "#     plt.xlabel('Principal components')\n",
        "#     plt.legend(loc='best')\n",
        "#     plt.tight_layout()\n",
        "\n",
        "#N=data.values\n",
        "# pca = PCA(n_components=20)\n",
        "# x = pca.fit_transform(X)\n",
        "# plt.figure(figsize = (5,5))\n",
        "# plt.scatter(x[:,0],x[:,1])\n",
        "# plt.show()\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "# kmeans = KMeans(n_clusters=3, random_state=5)\n",
        "# X_clustered = kmeans.fit_predict(X)\n",
        "\n",
        "# LABEL_COLOR_MAP = {0 : 'g',\n",
        "#                    1 : 'y',\n",
        "#                    2 : 'r'\n",
        "#                   }\n",
        "\n",
        "# label_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n",
        "# plt.figure(figsize = (5,5))\n",
        "# plt.scatter(x[:,0],x[:,1], c= label_color)\n",
        "# plt.show()\n",
        "\n",
        "# pca_modified=PCA(n_components=20)\n",
        "# pca_modified.fit_transform(X)\n",
        "print('##############################Task1###################################')\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=4)\n",
        "\n",
        "def print_score(classifier,X_train,y_train,X_test,y_test,train=True):\n",
        "    if train == True:\n",
        "        print(\"Training results:\\n\")\n",
        "        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_train,classifier.predict(X_train))))\n",
        "        #print('Classification Report:\\n{}\\n'.format(classification_report(y_train,classifier.predict(X_train))))\n",
        "        #print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_train,classifier.predict(X_train))))\n",
        "        res = cross_val_score(classifier, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\n",
        "        print(res)\n",
        "        print('Average Accuracy:\\t{0:.4f}\\n'.format(res.mean()))\n",
        "        print('Standard Deviation:\\t{0:.4f}'.format(res.std()))\n",
        "    elif train == False:\n",
        "        print(\"Test results:\\n\")\n",
        "        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_test,classifier.predict(X_test))))\n",
        "        #print('Classification Report:\\n{}\\n'.format(classification_report(y_test,classifier.predict(X_test))))\n",
        "        #print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_test,classifier.predict(X_test))))\n",
        "       \n",
        "\n",
        "def visualization_train(model):\n",
        "    sns.set_context(context='notebook',font_scale=2)\n",
        "    plt.figure(figsize=(10,6))\n",
        "    from matplotlib.colors import ListedColormap\n",
        "    X_set, y_set = X_train, y_train\n",
        "    X1, X2 = np.meshgrid(np.arange(start = X_set.min() - 1, stop = X_set.max() + 1, step = 0.01),\n",
        "                     np.arange(start = X_set.min() - 1, stop = X_set.max() + 1, step = 0.01))\n",
        "    # plt.contourf(X1, X2, classifier.predict().reshape(X1.shape), alpha = 0.6, cmap = ListedColormap(('red', 'green')))\n",
        "    # plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
        "            # alpha = 0.6, cmap = ListedColormap(('red', 'green')))\n",
        "    plt.xlim(X1.min(), X1.max())\n",
        "    plt.ylim(X2.min(), X2.max())\n",
        "    for i, j in enumerate(np.unique(y_set)):\n",
        "        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],X_set[y_set==j,2],\n",
        "                    c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
        "    plt.title(\"%s Training Set\" %(model))\n",
        "    plt.xlabel('PC 1')\n",
        "    plt.ylabel('PC 2')\n",
        "    plt.legend()\n",
        "def visualization_test(model):\n",
        "    sns.set_context(context='notebook',font_scale=2)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    from matplotlib.colors import ListedColormap\n",
        "    X_set, y_set = X_test, y_test\n",
        "    X1, X2 = np.meshgrid(np.arange(start = X_set.min() - 1, stop = X_set.max() + 1, step = 0.01),\n",
        "                         np.arange(start = X_set.min() - 1, stop = X_set.max() + 1, step = 0.01))\n",
        "    #plt.contourf(X1, X2, classifier.predict(np.array([X_test],[X_test]).reshape(X1.shape), alpha = 0.6, cmap = ListedColormap(('red', 'green','blue')))\n",
        "    #plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape), alpha = 0.6, cmap = ListedColormap(('red', 'green','blue')))\n",
        "    plt.xlim(X1.min(), X1.max())\n",
        "    plt.ylim(X2.min(), X2.max())\n",
        "    for i, j in enumerate(np.unique(y_set)):\n",
        "        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],X_set[y_set==j,2],\n",
        "                    c = ListedColormap(('red', 'green','blue'))(i), label = j)\n",
        "    plt.title(\"%s Test Set\" %(model))\n",
        "    plt.xlabel('PC 1')\n",
        "    plt.ylabel('PC 2')\n",
        "    plt.legend()\n",
        "\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# model_naive = GaussianNB()\n",
        "# model_naive.fit(X_train, y_train)\n",
        "\n",
        "# y_prob = model_naive.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \n",
        "# y_pred = np.where(y_prob > 0.5, 1, 0) # This will threshold the probabilities to give class predictions.\n",
        "# model_naive.score(X_test, y_pred)\n",
        "\n",
        "# print(\"Number of mislabeled points from %d points : %d\"\n",
        "#       % (X_test.shape[0],(y_test!= y_pred).sum()))\n",
        "\n",
        "# scores = cross_val_score(model_naive, X, y, cv=10, scoring='accuracy')\n",
        "# print(scores)\n",
        "\n",
        "# scores.mean()\n",
        "\n",
        "# confusion_matrix=metrics.confusion_matrix(y_test,y_pred)\n",
        "# confusion_matrix\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "\n",
        "knn = KNN()\n",
        "knn.fit(X_train,y_train)\n",
        "print('----------KNeighborClassifier----------')\n",
        "print_score(knn,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(knn,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC(kernel='rbf',random_state=42)\n",
        "\n",
        "svc.fit(X_train,y_train)\n",
        "print('----------SupportVectorClassifier----------')\n",
        "print_score(svc,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(svc,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "\n",
        "lr.fit(X_train,y_train)\n",
        "print('----------LogisticRegression----------')\n",
        "print_score(lr,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(lr,X_train,y_train,X_test,y_test,train=False)\n",
        "# visualization_train('Logistic Reg')\n",
        "# visualization_test('Logistic Reg')\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB as NB\n",
        "\n",
        "gnb = NB()\n",
        "gnb.fit(X_train,y_train)\n",
        "print('----------GaussianNaiveBayse----------')\n",
        "print_score(gnb,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(gnb,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier as DT\n",
        "\n",
        "dtc = DT(criterion='entropy',random_state=42)\n",
        "dtc.fit(X_train,y_train)\n",
        "print('----------DecisionTreeClassifier----------' )\n",
        "print_score(dtc,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(dtc,X_train,y_train,X_test,y_test,train=False)\n",
        "# visualization_train('Decision Tree')\n",
        "# visualization_test('Decision Tree')\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 42)\n",
        "rfc.fit(X_train, y_train)\n",
        "print('----------RandomForestClassifier----------')\n",
        "print_score(rfc,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(rfc,X_train,y_train,X_test,y_test,train=False)\n",
        "# visualization_train('Random Forest')\n",
        "# visualization_test('Random Forest')\n",
        "# import keras\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense\n",
        "\n",
        "# classifier = Sequential()\n",
        "\n",
        "# classifier.add(Dense(8, kernel_initializer='uniform', activation= 'relu', input_dim = 2))\n",
        "# classifier.add(Dense(6, kernel_initializer='uniform', activation= 'relu'))\n",
        "# classifier.add(Dense(5, kernel_initializer='uniform', activation= 'relu'))\n",
        "# classifier.add(Dense(4, kernel_initializer='uniform', activation= 'relu'))\n",
        "# classifier.add(Dense(1, kernel_initializer= 'uniform', activation= 'sigmoid'))\n",
        "# classifier.compile(optimizer= 'adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "filename = '/content/drive/My Drive/뇌및머신러닝/cls_model.pkl'\n",
        "pickle.dump(rfc, open(filename, 'wb'))\n",
        "\n",
        "print('###########################Task2###############################')\n",
        "from sklearn.model_selection import train_test_split,KFold,cross_val_score,GridSearchCV,RandomizedSearchCV\n",
        "from sklearn import metrics\n",
        "X = X[['ST116CV', 'ST23CV', 'ST24CV', 'ST44CV', 'ST83CV', 'ST91CV', 'ST24TA',\n",
        "       'ST31TA', 'ST32TA', 'ST35TA', 'ST40TA', 'ST64TA', 'ST83TA', 'ST91TA',\n",
        "       'ST99TA']]\n",
        "X=data.drop(columns=['DX_bl','ADAS11','ADAS13','MMSE'],axis=1) #Predictors\n",
        "y1=data['ADAS11'] #Response\n",
        "y2=data['ADAS13']\n",
        "y3=data['MMSE']\n",
        "X = X.interpolate(method='linear')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.1,random_state = 4)\n",
        "\n",
        "print(\"#############ADAS11###############\")\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lm1 = LinearRegression()\n",
        "lm1.fit(X_train,y_train)\n",
        "print('----------LinearRegression----------')\n",
        "print(lm1)\n",
        "print(lm1.intercept_)\n",
        "\n",
        "predictions = lm1.predict(X_test)\n",
        "\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,predictions)\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
        "score=cross_val_score(lm1,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtreg1 = DecisionTreeRegressor(random_state = 100)\n",
        "dtreg1.fit(X_train, y_train)\n",
        "dtr_pred = dtreg1.predict(X_test)\n",
        "print('----------DecisionTreeRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, dtr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, dtr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dtr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,dtr_pred,c='green')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(dtreg1,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "svr1 = SVR(kernel = 'rbf')\n",
        "svr1.fit(X_train, y_train)\n",
        "svr_pred = svr1.predict(X_test)\n",
        "print('----------SupportVectorRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, svr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, svr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,svr_pred, c='red')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(svr1,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfr1 = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "rfr1.fit(X_train, y_train)\n",
        "rfr_pred= rfr1.predict(X_test)\n",
        "print('----------RandomForestRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, rfr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, rfr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rfr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,rfr_pred, c='orange')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(rfr1,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "import lightgbm as lgb\n",
        "model_lgb1 = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
        "                              learning_rate=0.1, n_estimators=500,\n",
        "                              max_bin = 55, bagging_fraction = 0.8,\n",
        "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
        "model_lgb1.fit(X_train,y_train)\n",
        "lgb_pred = model_lgb1.predict(X_test)\n",
        "print('----------LGB----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, lgb_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, lgb_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, lgb_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,lgb_pred, c='orange')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(model_lgb1,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: %d',avscore)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1,random_state = 4)\n",
        "\n",
        "print(\"#############ADAS13###############\")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lm2 = LinearRegression()\n",
        "lm2.fit(X_train,y_train)\n",
        "print('----------LinearRegression----------')\n",
        "print(lm2)\n",
        "print(lm2.intercept_)\n",
        "\n",
        "predictions = lm2.predict(X_test)\n",
        "\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,predictions)\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
        "score=cross_val_score(lm2,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtreg2 = DecisionTreeRegressor(random_state = 100)\n",
        "dtreg2.fit(X_train, y_train)\n",
        "dtr_pred = dtreg2.predict(X_test)\n",
        "print('----------DecisionTreeRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, dtr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, dtr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dtr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,dtr_pred,c='green')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(dtreg2,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "svr2 = SVR(kernel = 'rbf')\n",
        "svr2.fit(X_train, y_train)\n",
        "svr_pred = svr2.predict(X_test)\n",
        "print('----------SupportVectorRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, svr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, svr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,svr_pred, c='red')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(svr2,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfr2 = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "rfr2.fit(X_train, y_train)\n",
        "rfr_pred= rfr2.predict(X_test)\n",
        "print('----------RandomForestRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, rfr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, rfr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rfr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,rfr_pred, c='orange')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(rfr2,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "import lightgbm as lgb\n",
        "model_lgb2 = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
        "                              learning_rate=0.1, n_estimators=500,\n",
        "                              max_bin = 55, bagging_fraction = 0.8,\n",
        "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
        "model_lgb2.fit(X_train,y_train)\n",
        "lgb_pred = model_lgb2.predict(X_test)\n",
        "print('----------LGB----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, lgb_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, lgb_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, lgb_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,lgb_pred, c='orange')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(model_lgb2,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: %d',avscore)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y3, test_size=0.1,random_state = 4)\n",
        "\n",
        "print(\"#############MMSE###############\")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lm3 = LinearRegression()\n",
        "lm3.fit(X_train,y_train)\n",
        "print('----------LinearRegression----------')\n",
        "print(lm3)\n",
        "print(lm3.intercept_)\n",
        "\n",
        "predictions = lm3.predict(X_test)\n",
        "\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,predictions)\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
        "score=cross_val_score(lm3,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dtreg3 = DecisionTreeRegressor(random_state = 100)\n",
        "dtreg3.fit(X_train, y_train)\n",
        "dtr_pred = dtreg3.predict(X_test)\n",
        "print('----------DecisionTreeRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, dtr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, dtr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, dtr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,dtr_pred,c='green')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(dtreg3,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "svr3 = SVR(kernel = 'rbf')\n",
        "svr3.fit(X_train, y_train)\n",
        "svr_pred = svr3.predict(X_test)\n",
        "print('----------SupportVectorRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, svr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, svr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, svr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,svr_pred, c='red')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(svr3,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfr3 = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
        "rfr3.fit(X_train, y_train)\n",
        "rfr_pred= rfr3.predict(X_test)\n",
        "print('----------RandomForestRegressor----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, rfr_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, rfr_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rfr_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,rfr_pred, c='orange')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(rfr3,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: ',avscore)\n",
        "\n",
        "import lightgbm as lgb\n",
        "model_lgb3 = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
        "                              learning_rate=0.1, n_estimators=500,\n",
        "                              max_bin = 55, bagging_fraction = 0.8,\n",
        "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
        "                              feature_fraction_seed=9, bagging_seed=9,\n",
        "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
        "model_lgb3.fit(X_train,y_train)\n",
        "lgb_pred = model_lgb3.predict(X_test)\n",
        "print('----------LGB----------')\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, lgb_pred))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, lgb_pred))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, lgb_pred)))\n",
        "# plt.figure(figsize=(15,8))\n",
        "# plt.scatter(y_test,lgb_pred, c='orange')\n",
        "# plt.xlabel('Y Test')\n",
        "# plt.ylabel('Predicted Y')\n",
        "# plt.show()\n",
        "score=cross_val_score(model_lgb3,X_train,y_train,cv=10)\n",
        "print(score)\n",
        "avscore = np.mean(score)\n",
        "print('average train score: %d',avscore)\n",
        "# print_score(classifier,X_train,y_train,X_test,y_test,train=True)\n",
        "# print_score(classifier,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "filename = '/content/drive/My Drive/뇌및머신러닝/reg_model_1.pkl'\n",
        "pickle.dump(rfr1, open(filename, 'wb'))\n",
        "filename = '/content/drive/My Drive/뇌및머신러닝/reg_model_2.pkl'\n",
        "pickle.dump(rfr2, open(filename, 'wb'))\n",
        "filename = '/content/drive/My Drive/뇌및머신러닝/reg_model.pkl'\n",
        "pickle.dump(rfr3, open(filename, 'wb'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "##############################Task1###################################\n",
            "----------KNeighborClassifier----------\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.6687\n",
            "\n",
            "[0.53793103 0.53103448 0.54482759 0.55862069 0.55172414 0.50344828\n",
            " 0.46527778 0.47916667 0.47916667 0.49305556]\n",
            "Average Accuracy:\t0.5144\n",
            "\n",
            "Standard Deviation:\t0.0325\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.4907\n",
            "\n",
            "----------SupportVectorClassifier----------\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.6286\n",
            "\n",
            "[0.63448276 0.53793103 0.57931034 0.51724138 0.56551724 0.52413793\n",
            " 0.54166667 0.54166667 0.56944444 0.56944444]\n",
            "Average Accuracy:\t0.5581\n",
            "\n",
            "Standard Deviation:\t0.0322\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.5404\n",
            "\n",
            "----------LogisticRegression----------\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.5373\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.52413793 0.51034483 0.53103448 0.55172414 0.51724138 0.48965517\n",
            " 0.52777778 0.47222222 0.53472222 0.48611111]\n",
            "Average Accuracy:\t0.5145\n",
            "\n",
            "Standard Deviation:\t0.0236\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.5590\n",
            "\n",
            "----------GaussianNaiveBayse----------\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.4903\n",
            "\n",
            "[0.54482759 0.53103448 0.46896552 0.45517241 0.51724138 0.48275862\n",
            " 0.43055556 0.45833333 0.51388889 0.47916667]\n",
            "Average Accuracy:\t0.4882\n",
            "\n",
            "Standard Deviation:\t0.0351\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.4907\n",
            "\n",
            "----------DecisionTreeClassifier----------\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 1.0000\n",
            "\n",
            "[0.54482759 0.44137931 0.54482759 0.40689655 0.55172414 0.43448276\n",
            " 0.45138889 0.45138889 0.47916667 0.53472222]\n",
            "Average Accuracy:\t0.4841\n",
            "\n",
            "Standard Deviation:\t0.0519\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.5155\n",
            "\n",
            "----------RandomForestClassifier----------\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 1.0000\n",
            "\n",
            "[0.62068966 0.57931034 0.53103448 0.55862069 0.55862069 0.50344828\n",
            " 0.52083333 0.52083333 0.56944444 0.52083333]\n",
            "Average Accuracy:\t0.5484\n",
            "\n",
            "Standard Deviation:\t0.0338\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.5031\n",
            "\n",
            "###########################Task2###############################\n",
            "#############ADAS11###############\n",
            "----------LinearRegression----------\n",
            "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
            "46.3143837830828\n",
            "MAE: 4.215682758708425\n",
            "MSE: 30.528737898818726\n",
            "RMSE: 5.52528170311874\n",
            "[0.36712634 0.30812276 0.38786793 0.29038575 0.40767386 0.23424181\n",
            " 0.19616565 0.47892054 0.36981345 0.48345859]\n",
            "average train score:  0.35237766882702826\n",
            "----------DecisionTreeRegressor----------\n",
            "MAE: 5.689503105590062\n",
            "MSE: 55.87466521739131\n",
            "RMSE: 7.4749358002187085\n",
            "[-0.19822142 -0.32856622 -0.31352286 -0.0625976   0.0741579  -0.43525687\n",
            " -0.56897813 -0.08066609 -0.12086272  0.0283891 ]\n",
            "average train score:  -0.20061249047376234\n",
            "----------SupportVectorRegressor----------\n",
            "MAE: 4.830971075104906\n",
            "MSE: 41.49252903743505\n",
            "RMSE: 6.441469478110958\n",
            "[0.09365024 0.0438767  0.16341408 0.14464107 0.17839616 0.11896729\n",
            " 0.15407362 0.2240644  0.08932098 0.08264817]\n",
            "average train score:  0.12930527087482496\n",
            "----------RandomForestRegressor----------\n",
            "MAE: 4.114024223602485\n",
            "MSE: 26.709533812857146\n",
            "RMSE: 5.168126721826502\n",
            "[0.36151062 0.35510426 0.43986002 0.35041562 0.51934288 0.30011508\n",
            " 0.32074334 0.48984766 0.40504261 0.48031232]\n",
            "average train score:  0.402229440543252\n",
            "----------LGB----------\n",
            "MAE: 4.2065856963547965\n",
            "MSE: 29.305930501070247\n",
            "RMSE: 5.4134952203793665\n",
            "[0.38253588 0.34357363 0.42722147 0.39263752 0.42789075 0.23428324\n",
            " 0.25949337 0.49257836 0.38625519 0.52188095]\n",
            "average train score: %d 0.3868350354713435\n",
            "#############ADAS13###############\n",
            "----------LinearRegression----------\n",
            "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
            "70.67934372218036\n",
            "MAE: 5.840654307718017\n",
            "MSE: 57.496694341130315\n",
            "RMSE: 7.582657472227683\n",
            "[0.39679567 0.36291186 0.45848115 0.34131631 0.47313114 0.32478374\n",
            " 0.27914706 0.52100795 0.40703488 0.50504835]\n",
            "average train score:  0.4069658097087784\n",
            "----------DecisionTreeRegressor----------\n",
            "MAE: 8.534099378881988\n",
            "MSE: 111.65986770186336\n",
            "RMSE: 10.566923284564119\n",
            "[-0.18553469 -0.35212735 -0.20704421 -0.12350632 -0.24388616 -0.33572025\n",
            " -0.34685552 -0.2008542  -0.19003343 -0.05888558]\n",
            "average train score:  -0.22444476949576994\n",
            "----------SupportVectorRegressor----------\n",
            "MAE: 7.226895596787794\n",
            "MSE: 84.91503427775399\n",
            "RMSE: 9.214935391946815\n",
            "[0.10447325 0.0792876  0.16474915 0.17731407 0.21028906 0.15016231\n",
            " 0.17685233 0.2391361  0.11681346 0.10904973]\n",
            "average train score:  0.15281270483494663\n",
            "----------RandomForestRegressor----------\n",
            "MAE: 5.5736105590062115\n",
            "MSE: 51.08871040925465\n",
            "RMSE: 7.147636700984084\n",
            "[0.39512085 0.39671101 0.45481479 0.39448627 0.55297604 0.32949014\n",
            " 0.39560822 0.52973621 0.40208814 0.51527398]\n",
            "average train score:  0.43663056290597035\n",
            "----------LGB----------\n",
            "MAE: 5.824283310219243\n",
            "MSE: 59.00053891176344\n",
            "RMSE: 7.681180827956301\n",
            "[0.42808819 0.40917274 0.47933797 0.44109295 0.44664367 0.32866873\n",
            " 0.29969256 0.52648453 0.42993694 0.53816419]\n",
            "average train score: %d 0.4327282466354494\n",
            "#############MMSE###############\n",
            "----------LinearRegression----------\n",
            "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
            "13.59371067228008\n",
            "MAE: 1.7443654416697738\n",
            "MSE: 5.066270375017576\n",
            "RMSE: 2.250837705170583\n",
            "[0.33432889 0.20820225 0.27960735 0.29314491 0.20872758 0.09894836\n",
            " 0.19171891 0.38002469 0.22320239 0.41877008]\n",
            "average train score:  0.26366754238627244\n",
            "----------DecisionTreeRegressor----------\n",
            "MAE: 1.9937888198757765\n",
            "MSE: 7.509316770186335\n",
            "RMSE: 2.740313261323664\n",
            "[-0.34252863 -0.38795045 -0.16806038 -0.22035992 -0.22898127 -0.49840833\n",
            " -0.46740966 -0.16741271 -0.21468633 -0.36577127]\n",
            "average train score:  -0.30615689423737036\n",
            "----------SupportVectorRegressor----------\n",
            "MAE: 1.9786457271090834\n",
            "MSE: 6.704480881142885\n",
            "RMSE: 2.589301234144626\n",
            "[0.14516775 0.072805   0.13715534 0.18055473 0.19842922 0.14867178\n",
            " 0.11748734 0.25784472 0.07824977 0.16242172]\n",
            "average train score:  0.14987873866125562\n",
            "----------RandomForestRegressor----------\n",
            "MAE: 1.697391304347826\n",
            "MSE: 4.760108074534162\n",
            "RMSE: 2.1817671907273155\n",
            "[0.33806684 0.33201271 0.36846327 0.39160393 0.3522579  0.18034942\n",
            " 0.28648844 0.38696723 0.29599852 0.36650206]\n",
            "average train score:  0.3298710322189904\n",
            "----------LGB----------\n",
            "MAE: 1.757623112249006\n",
            "MSE: 5.034799928795405\n",
            "RMSE: 2.2438359852706267\n",
            "[0.37432724 0.30479952 0.37649879 0.40017027 0.25248829 0.14352272\n",
            " 0.18886465 0.36928586 0.26203185 0.41436417]\n",
            "average train score: %d 0.30863533570809426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQI5omaSq9zo"
      },
      "source": [
        "<head><font size=6>Midterm Project <font><br></head>\n",
        "\n",
        "1. <font size=4 bold>Task1<br><p>\n",
        "    a. Data extraction <br>\n",
        "    The data given has 140 features therefore extracting data that is important than other feature is important. Also in order to train models, I splitted the data in to train and test. With the first column DS_dl for supervised classifications. To find the important features, I used PCA( principle component analysis to find the number of features important. I concluded that about 15 features were most important and found these columns respectively. <br><p>\n",
        "    b. Two functions for scores and visualization<br>\n",
        "    Since I will use a couple of models for the data, there needed to be standard scoring function to find the 10fold cross validation, accuary of the predictions, and classification scores. Also for visualization, I implemented a simple dot marking grid to find each of the data. However the datas were hard to find a pattern as visualized, which might have been the reason for low scores.<br><p>\n",
        "    c. K- neighbor Classifier <br><p>\n",
        "    d. Support Vector Classifier <br><p>\n",
        "    e. Logistic Regression<br><p>\n",
        "    f. Naive Bayes Classifier<br><p>\n",
        "    g. Decision Tree <br><p>\n",
        "    h. Random Forest Classifier<br><p>\n",
        "2. <font size=4>Task2<br><p>\n",
        "    a. Data extraction <br>\n",
        "    Similar to the classification I needed to find a regression model that had the best score. This time the score function would be MSE(mean squared error) with the prediction and original data sets. Instead of using the supervised sets as one column, I decided to used three columns sperately as the higher the percentage of accuracy<p>\n",
        "    b. Linear Regression <br><p>\n",
        "    c. Support Vector Regressor<br><p>\n",
        "    d. Random Forest Regressor<br><p>\n",
        "    e. Light GBM <br><p>\n",
        "4. <font size=4>Model Comparison<br><p>\n",
        "    a. Task1<br><p>\n",
        "    From the 6 models that I compared, two of the models were most accurate. Calculating the mena from cross validation with 10folds. The SVM averaged around 60.05% in test sets, while the RFC averaged around 55.1%. Other models were camparable low not able to go above 50% in average. Also there was a slight difference with the number of features that was used. When using all 140 columns for features. The average accuaracy was lower compared to using only 15 features. <br><p>\n",
        "    b. Task2<br><p>\n",
        "    4 models were compared in the regression part. The Random forest regressor always had the highes accuracy compared to other three models. The svr expecially had poor results sometimes showing negative results. \n",
        "\n",
        "5. <font size=4>Discussion<br><p>\n",
        "    It is notedly seen that the accuaracy of the prediction does not go above 60%. Having a lot of features. I believe it is important to find features that is important and is critical in using for classification. However even though I have figured about 41 features are most related. The accuaracy of the whole models did not improve as much as wanted. Therefore rigorously finding out the maximum accuracy for the given features, I concluded that 15 most promising features leaded to the hightes accuaracy. Random forest classification and suppport vector classification had the highest score in classification. Random forest regressor had the highest score in regression. <br><p>\n",
        "6. Conclusion<br><p>\n",
        "    This project was very eye opening to the machine learning field with datasets. The implementation was rigorous but also quite fun to do. As we are learning deep learning parts, I would further wish to study Neural networks, such as CNN and try implementing it. I tried to use NN in this project, however it was hard to access the data and form the layers. This project would have had a better accuaracy score, if deeplearning was used and different models were also thought through. Also in regression, my way of implementing nan's were using linear values from the near by datasets. However if I could have built a model that fills in nan datas specifically using regression based on the whole dataset would have had a more accurate score. "
      ]
    }
  ]
}
